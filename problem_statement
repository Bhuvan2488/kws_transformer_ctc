The objective is to build a speech alignment model using a Transformer Encoder with CTC loss that learns from audio files, their exact transcriptions, and annotation files. The model will take an audio file and its corresponding text as input and automatically predict the start and end timestamps for each word in the audio. The model is designed to perform word-level forced alignment, enabling precise timestamp extraction for every word spoken in the audio, using a Transformer-based acoustic encoder trained with CTC for monotonic sequence alignment.