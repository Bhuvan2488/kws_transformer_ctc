The objective is to build a speech alignment model using a Transformer Encoder trained directly on audio files and their Audacity annotation files. During training, the model learns word-level timing by converting Audacity word start and end timestamps into frame-level supervision. Audio is converted into log-mel acoustic features, and the Transformer produces frame-wise word label predictions (including a BLANK/non-word class) and is trained using CrossEntropyLoss. After training and deployment, the system will take an audio file and a target word as inputs and return the predicted start and end timestamps of that word in the audio (or report that the word is not present). This enables word-level forced alignment and precise keyword timestamp extraction using only audio and annotation-based supervision.
