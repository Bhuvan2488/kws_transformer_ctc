The objective is to build a speech alignment model using a Transformer Encoder trained directly on audio files and their Audacity annotation files. The model will take an audio file as input and learn to predict word-level timing information by converting annotated word start and end timestamps into frame-level supervision. Using log-mel acoustic features, the Transformer will output frame-wise word labels (including a blank/non-word class) and will be trained using CrossEntropyLoss. The final system is designed to perform word-level forced alignment by producing accurate start and end timestamps for each spoken word in the audio, based purely on audio and annotation-based supervision.