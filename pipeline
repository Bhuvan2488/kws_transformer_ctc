#!/bin/bash

# ============================================================
# FINAL PIPELINE — kws_transformer_ctc
# ============================================================

# ------------------------------------------------------------
# STEP 1: DATA INGESTION
# ------------------------------------------------------------
# Input folders:
# data/raw/audio/        → .wav files
# data/raw/transcripts/  → exact text files
# data/raw/annotations/  → _Annotated.txt files
#
# Used by:
# - src/data/audio_loader.py
# - src/data/text_loader.py


# ------------------------------------------------------------
# STEP 2: DATA VALIDATION (FINAL RULE)
# ------------------------------------------------------------
# ONLY RULE:
# If annotation file content == "#"
#     → discard sample
# Else
#     → keep sample
#
# NO OTHER CHECKS:
# - No transcript vs audio validation
# - No spelling validation
# - No word count check
# - No audio length filtering
#
# Files involved:
# - src/data/audio_loader.py
# - src/data/text_loader.py
# - src/data/dataset.py


# ------------------------------------------------------------
# STEP 3: FEATURE EXTRACTION
# ------------------------------------------------------------
# File:
# src/data/feature_extraction.py
#
# Process:
# - Load audio
# - Extract log-mel or MFCC
# - Normalize features
#
# Output:
# data/processed/features/


# ------------------------------------------------------------
# STEP 4: TEXT TOKENIZATION
# ------------------------------------------------------------
# File:
# src/data/text_loader.py
#
# Process:
# - Convert text to characters
# - Add CTC blank token
# - Encode tokens numerically
#
# Output:
# data/processed/tokenized_text/


# ------------------------------------------------------------
# STEP 5: DATASET PREPARATION
# ------------------------------------------------------------
# File:
# src/data/dataset.py
#
# Creates:
# - (audio_features, token_ids)
# - padding and masks
#
# Uses split files:
# data/splits/train.txt
# data/splits/val.txt
# data/splits/test.txt


# ------------------------------------------------------------
# STEP 6: MODEL ARCHITECTURE
# ------------------------------------------------------------
# Directory:
# src/model/
#
# Files:
# - transformer_encoder.py
# - ctc_head.py
# - model.py
#
# Flow:
# Audio → Transformer Encoder → Linear Layer → CTC Loss


# ============================================================
# STEP 7: TRAINING  (EXPLICIT & COMPLETE)
# ============================================================

# ------------------------------------------------------------
# PURPOSE
# ------------------------------------------------------------
# Train Transformer + CTC model using prepared dataset

# ------------------------------------------------------------
# DIRECTORIES USED
# ------------------------------------------------------------
# src/training/
# src/model/
# src/data/

# ------------------------------------------------------------
# FILES INVOLVED (ALL)
# ------------------------------------------------------------

# DATA
# src/data/dataset.py
#   - SpeechDataset
#   - collate_fn

# MODEL
# src/model/transformer_encoder.py
# src/model/ctc_head.py
# src/model/model.py

# TRAINING
# src/training/train.py        → main training loop
# src/training/optimizer.py    → optimizer (Adam / AdamW)
# src/training/scheduler.py    → LR scheduler (optional)
# src/training/loss.py         → NOT USED (CTC inside model)

# ------------------------------------------------------------
# INPUT FILES / FOLDERS
# ------------------------------------------------------------
# data/processed/features/        → .npy log-mel features
# data/processed/tokenized_text/ → tokenized transcripts (.json)
# data/splits/train.txt
# data/splits/val.txt

# ------------------------------------------------------------
# DATA FLOW (NO AMBIGUITY)
# ------------------------------------------------------------
# SpeechDataset (dataset.py)
#   → DataLoader (collate_fn from dataset.py)
#   → batch:
#       - audio
#       - tokens
#       - audio_lengths
#       - token_lengths
#   → KWSCTCModel.forward()
#   → CTCLoss (inside model.py)
#   → backward()
#   → optimizer.step()

# ------------------------------------------------------------
# OUTPUTS
# ------------------------------------------------------------
# outputs/checkpoints/   → model_epoch_*.pt
# outputs/logs/          → loss logs

# ------------------------------------------------------------
# EXECUTION
# ------------------------------------------------------------
# python src/training/train.py


# ============================================================
# STEP 8: CTC DECODING (EXPLICIT & FINAL)
# ============================================================
# ------------------------------------------------------------
# PURPOSE
# ------------------------------------------------------------
# Convert frame-level CTC model outputs into a clean
# character sequence with frame-level alignment info,
# to enable word-level timestamp extraction in Step 9.
# ------------------------------------------------------------
# DIRECTORIES USED
# ------------------------------------------------------------
# src/inference/
# ------------------------------------------------------------
# FILES INVOLVED
# ------------------------------------------------------------
# src/inference/ctc_decode.py
# ------------------------------------------------------------
# INPUTS
# ------------------------------------------------------------
# - Logits from trained KWSCTCModel
# Shape: (T, vocab_size) or (B, T, vocab_size)
# - vocab.json (for blank token index)
# ------------------------------------------------------------
# OUTPUTS
# ------------------------------------------------------------
# - Decoded character sequence
# - Frame-aligned character spans
#
# Example output (per sample):
# [
#   (char_id, start_frame, end_frame),
#   ...
# ]
# ------------------------------------------------------------
# DECODING RULES (STRICT CTC)
# ------------------------------------------------------------
# 1. Apply argmax over vocabulary for each time frame
# 2. Collapse consecutive repeated tokens
# 3. Remove blank tokens (AFTER collapsing)
# 4. Preserve frame indices for alignment
# ------------------------------------------------------------
# CRITICAL ASSUMPTIONS
# ------------------------------------------------------------
# - Blank token index MUST match training (usually 0)
# - Greedy decoding ONLY (NO beam search, NO LM)
# - Decoding is character-level
# ------------------------------------------------------------
# DATA FLOW
# ------------------------------------------------------------
# Model logits
# → argmax over vocab
# → collapse repeats
# → remove blanks
# → emit (char_id, start_frame, end_frame)
# ------------------------------------------------------------
# WHY THIS FORMAT IS REQUIRED
# ------------------------------------------------------------
# - Frame spans are mandatory for Step 9
# - Word boundaries are built from character spans
# - Timestamps are computed using frame → time mapping
# ------------------------------------------------------------
# EXECUTION
# ------------------------------------------------------------
# Called inside:
# - src/inference/align.py
# - scripts/infer_alignment.py
# ------------------------------------------------------------
# OUTPUT CONSUMERS
# ------------------------------------------------------------
# - src/inference/timestamp_extractor.py
# - outputs/predictions/aligned_words.json
# ------------------------------------------------------------
# PIPELINE COMPATIBILITY
# ------------------------------------------------------------
# - Fully compatible with Step 7 implementation
# - No dependency on optimizer, scheduler, or model size
# - Stable for CPU and GPU inference
# ------------------------------------------------------------
# END OF STEP 8
# ------------------------------------------------------------


# ------------------------------------------------------------
# STEP 9: WORD-LEVEL ALIGNMENT
# ------------------------------------------------------------
# Files:
# - src/inference/align.py
# - src/inference/timestamp_extractor.py
#
# Process:
# - Map CTC frames to characters
# - Group characters into words
# - Extract start and end timestamps
#
# Output:
# outputs/predictions/aligned_words.json


# ------------------------------------------------------------
# STEP 10: EVALUATION
# ------------------------------------------------------------
# Directory:
# src/evaluation/
#
# Files:
# - wer.py
# - alignment_metrics.py
#
# Metrics:
# - Word Error Rate
# - Timestamp deviation


# ------------------------------------------------------------
# STEP 11: EXECUTION SCRIPTS
# ------------------------------------------------------------
# scripts/preprocess.py       → steps 1–5
# scripts/train.py            → training
# scripts/evaluate.py         → evaluation
# scripts/infer_alignment.py  → inference


# ------------------------------------------------------------
# STEP 12: FINAL OUTPUT
# ------------------------------------------------------------
# outputs/
# ├── checkpoints/
# ├── logs/
# └── predictions/
#     └── aligned_words.json
#
# ------------------------------------------------------------
# END OF PIPELINE
# ------------------------------------------------------------
